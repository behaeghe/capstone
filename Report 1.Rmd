---
title: 'Coursera Capstone EDA - Report #1'
author: "JRB"
date: "October 30, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align = "center")

library(tidyverse) ##Because the universe is a better place with tidyverse
library(tidytext)
library(tm)
library(ggraph)
library(igraph)
library(knitr) #For table formatting
library(gridExtra)
library(ggthemes)

ggstyle <- function() {
  theme_tufte() +
  theme(
                text = element_text(
                                        face="italic",
                                        colour="black", 
                                        size=10)
                        )

} 
```

##Introduction

##Data Aquistion  

We are downloading and unziping the zip file provided by Coursera. We are also downloading a list of [profanities]("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en") to be used later to clean up the data set. We then proceed to create a data frame corp, which contains one observation per line of text and each observation as three variables: 
 
* *corp*: the origin of the text (twitter, news or blogs)    
* *line*: line number   
* *text*: the line of text itself from the text    

At the end of data acquisition we have a single data frame ```corp``` that would be used to performe our exploratory data analysis.

```{r Data Aquisition, warning=FALSE}
library(httr)
url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if   (!dir.exists("./data")) {
      dir.create("./data")   
 }
if(!file.exists("./data/swiftkey.zip")){
         GET(url,write_disk("./data/swiftkey.zip",overwrite = TRUE))
         unzip("./data/swiftkey.zip")
 }  
if(!file.exists("./data/profanity.txt")) {
        url = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
        GET(url,write_disk("./data/profanity.txt"))
}
        
#blogs <- readLines("./final/en_US/en_US.blogs.txt")
#news <- readLines("./final/en_US/en_US.news.txt")
twitter <- readLines("./final/en_US/en_US.twitter.txt")
## This is is our "profanity" list
swears <- readLines("./data/profanity.txt")
 ## Prepping our list of swear words
swears <- as_tibble(as.character(swears))
colnames(swears) <- "word"
##Creating a corpus for our exploratory data analysis
#blogs_df <- data_frame(corp="blogs",line=1:length(blogs),text=blogs)
#news_df <- data_frame(corp="news",line=1:length(news),text=news)
twitter_df <- data_frame(corp="twitter",line=1:length(twitter),text=twitter)
#corp <- rbind(blogs_df,news_df,twitter_df)
###
```

##Data Preparation

A few simple steps to prepare the data for analysis.  
 
* Step 1: select the twitter data only
* Step 2: Normalize the case, clean any twitter artifacts (e.g. "rt") and remove any numbers in the text
* Step 3: tokenize the text by words and eliminateany profanity	

This should be enough preparation for our exploratory analysis 

```{r data preparation}
#Step 1: subset for twitter
#corp <-twitter_df
corp <- sample_n(twitter_df,10000)
#Step 2: normalizing case, cleaning twitter artifacts and removing digits
corp$text <- tolower(corp$text)
corp$text <- gsub("rt","",corp$text) #Remove rt, retweet
corp$text <- gsub("@\\w+", "", corp$text) #Remoeve any @
corp$text <- gsub("[[:digit:]]", "", corp$text) # remove numbers

#Step 3: removing stop words (a, the, etc...) and profanity
data(stop_words)
tidy_corp_nostop <- corp %>%
                unnest_tokens(word,text,token="words") %>%
                anti_join(swears, by="word") %>%
                anti_join(stop_words, by="word")

tidy_corp_stop <- corp %>%
                unnest_tokens(word,text,token="words") %>%
                anti_join(swears, by="word")

tidy_corp_nostop_summary <- tidy_corp_nostop %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  mutate(corp="twitter") %>%
  bind_tf_idf(word,corp,n)

#tidy_corp_nostop_summary <- tidy_corp_nostop_summary %>%  mutate(cumsum=cumsum(n)) %>% mutate(cumsumfreq=cumsum(tf))

tidy_corp_stop_summary <- tidy_corp_stop %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  mutate(corp="twitter") %>%
  bind_tf_idf(word,corp,n)

#tidy_corp_stop_summary <- tidy_corp_stop_summary %>%  mutate(cumsum=cumsum(n)) %>% mutate(cumsumfreq=cumsum(tf))

```


##Exploratory Data Analysis
### Summary Statistics
```{r as.is= TRUE}
  number_of_lines <- max(corp$line) 
  number_of_words <- length(tidy_corp_nostop$word)
  distinct_words <- length(tidy_corp_nostop_summary$word)
  mySummary <- data.frame(lines= number_of_lines,words=number_of_words,distinct=distinct_words)
  kable(mySummary)
```

###Part 1: monogram analysis  

```{r word count plot, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
corp_freq <- tidy_corp_stop_summary %>% 
              bind_tf_idf(word,corp,n)
g1 <- corp_freq %>% filter(rank(desc(tf))<=25) %>%
  ggplot(aes(word, tf)) +
  geom_point() +
  xlab(NULL) +
  ylab("Frequency") +
  ggtitle("25 most frequent words \n including stop words") +
  coord_flip() + 
  ggstyle()
  
corp_freq_nostop <- tidy_corp_nostop_summary %>% 
              bind_tf_idf(word,corp,n) %>%
              mutate(cumsum=cumsum(tf))

g2 <- corp_freq_nostop %>%  filter(rank(desc(tf))<=25) %>%
  ggplot(aes(word, tf)) +
  geom_point() +
  xlab(NULL) +
  ylab("Frequency") +
  ggtitle("25 most frequent words \n excluding stop words") +
  coord_flip() +
  ggstyle()
  grid.arrange(g1,g2,ncol=2)
```
--

####Word cloud
Word cloud showing the relative importance of the 125 most frequent words in the twiter corpus

```{r word cloud}
library(wordcloud)
#freq_word <- bind_tf_idf(count_word,word,orig,n)
wordcloud(corp_freq_nostop$word,freq=corp_freq_nostop$tf,max.words=125,colors=brewer.pal(9,"RdPu"))
```
--

###Part 2: n-gram analysis

####n-grams analysis
```{r 2-grams}

corp_bigram <- unnest_tokens(corp,ngram,text,token="ngrams",n=2)
corp_bigram <- corp_bigram %>%
          separate(ngram,c("word1","word2"),sep=" ")
#corp_bigram <- corp_bigram %>%
#              filter(!word1 %in% stop_words$word) %>%
#              filter(!word2 %in% stop_words$word)
corp_bigram_count <- corp_bigram %>%
          unite(bigram,word1, word2, sep=" ") %>%
          count(bigram, sort=TRUE) %>%
          mutate(orig="twitter") %>%
          arrange(desc(n))
## TO DO: Bigram are factors, need to either make it a string or re-order the factors.
corp_bigram_count <- corp_bigram_count %>% 
                    bind_tf_idf(bigram,orig,n) %>%
                    mutate(bigram=factor(bigram,levels=bigram))
bigram_graph <- corp_bigram_count %>% 
                filter(rank(desc(n)) <= 10) %>%
                graph_from_data_frame()
g1 <-  corp_bigram_count %>% 
  filter(rank(desc(tf))<= 25) %>%
  ggplot(aes(reorder(bigram,desc(bigram)),tf)) +
  geom_point(stat="identity") + 
  xlab(NULL) +
  coord_flip()+
  ylab("Frequency") +
  ggtitle("25 most frequent bigrams") +
   theme_minimal() +
  theme(
                text = element_text(
                                        face="italic",
                                        colour="black", 
                                        size=8)
                        )

```

```{r 3-grams}
corp_trigram <- unnest_tokens(corp,ngram,text,token="ngrams",n=3)
corp_trigram <- corp_trigram %>%
          separate(ngram,c("word1","word2","word3"),sep=" ")
corp_trigram_count <- corp_trigram %>%
          unite(trigram,word1, word2,word3, sep=" ") %>%
          count(trigram, sort=TRUE) %>%
          mutate(orig="twitter") %>%
          arrange(desc(n))
corp_trigram_count <- corp_trigram_count %>% 
                    bind_tf_idf(trigram,orig,n) %>%
                    mutate(trigram=factor(trigram,levels=trigram))

g2 <-  corp_trigram_count %>% 
  filter(rank(desc(tf))<= 25) %>%
  ggplot(aes(reorder(trigram,desc(trigram)),tf)) +
  geom_point(stat="identity") + 
  xlab(NULL) +
  coord_flip()+
  ylab("Frequency") +
  ggtitle("25 most frequent trigrams") +
    theme_minimal() +
  theme(
                text = element_text(
                                        face="italic",
                                        colour="black", 
                                        size=8)
                        )
grid.arrange(g1,g2,ncol=2)
```
```{r graph analysis}
#bigram

bigram_graph <- corp_bigram %>% 
                count(word1,word2,sort=TRUE) %>%
                filter(rank(desc(n)) <= 125) %>%
                graph_from_data_frame() 
a <- grid::arrow(type="closed",unit(0.15,"inches"))
ggraph(bigram_graph, layout = "fr") +
      geom_edge_link(aes(edge_alpha=n),
                  show.legend = FALSE,
                  arrow=a,end_cap=circle(0.07,"inches") ) +
      geom_node_point(color="lightblue",size=5) +
      geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
      theme_void()
#Trigram


trigram_graph <- corp_trigram %>% 
                count(word1,word2,word3,sort=TRUE) %>%
                filter(rank(desc(n)) <= 125) %>%
                graph_from_data_frame() 
a <- grid::arrow(type="closed",unit(0.15,"inches"))
ggraph(trigram_graph, layout = "fr") +
      geom_edge_link(aes(edge_alpha=n),
                  show.legend = FALSE,
                  arrow=a,end_cap=circle(0.07,"inches") ) +
      geom_node_point(color="lightblue",size=5) +
      geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
      theme_void()
```
###Conclusions and Next Steps
#References:
[How to clean twitter data in r](http://technokarak.com/how-to-clean-the-twitter-data-using-r-twitter-mining-tutorial.html)
[Text mining in R: the tidy approach](http://tidytextmining.com/)