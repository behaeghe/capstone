---
title: "Capstone Project for Data Science Specialization"
output:
 
  html_document: default
date: "Sep 11 2017"
---

# Acquiring data

```{r}
        library(httr)
        url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
 if   (!dir.exists("./data")) {
      dir.create("./data")   
 }
 if(!file.exists("./data/swiftkey.zip")){
         GET(url,write_disk("./data/swiftkey.zip",overwrite = TRUE))
         unzip("./data/swiftkey.zip")
         }       
```
# Loading and tyding the data in R

```{r creating corpa, message=TRUE, warning=TRUE, include=FALSE}
        library(dplyr)
        library(tidytext)
        blogs <- readLines("./data/final/en_US/en_US.blogs.txt")
        news <- readLines("./data/final/en_US/en_US.news.txt")
        twitter <- readLines("./data/final/en_US/en_US.twitter.txt")
        url = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
        GET(url,write_disk("./data/profanity.txt"))
        swears <- as.data.frame(readLines("./data/profanity.txt"))
        colnames(swears) <- c("text")
##Creating a sample corpus to try things out
blogs_df <- data_frame(corp="blogs",line=1:length(blogs),text=blogs)
news_df <- data_frame(corp="news",line=1:length(news),text=news)
twitter_df <- data_frame(corp="twitter",line=1:length(twitter),text=twitter)

sample_corp <- rbind(blogs_df,news_df,twitter_df)
###

```
# Using tidy to make it nice: ["Text mining with R"](http://tidytextmining.com)
## Loading data, pre-processing and creating tidy text data set with word counts
```{r loading data, echo=TRUE, message=TRUE}
library(dplyr)
library(tidytext)
library(tm)
library(ggplot2)
# Maek it tidy, one word per row
# Some preprocessing
#subsetting for twitter
sample_corp <- subset(sample_corp,corp=="twitter")
sample_corp$text <- tolower(sample_corp$text)
sample_corp$text <- removeNumbers(sample_corp$text) #if package not available, could use a regexp
sample_corp$text <- removePunctuation(sample_corp$text,preserve_intra_word_dashes = TRUE)
## Prepping swear list
tidy_swears <- as_tibble(as.character(swears))
colnames(tidy_swears) <- "word"
tidy_swears <- as_tibble(as.character(swears))
colnames(tidy_swears) <- "word"
##
corp_words <- sample_corp %>%
        unnest_tokens(word,text) %>%
        anti_join(tidy_swears) %>%
        group_by(word,corp) %>%
        summarize(n=n()) %>%
        ungroup() %>%
        arrange(desc(n))
```
## Exploring our text data set
The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

Tasks to accomplish

Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.
Questions to consider

Some words are more frequent than others - what are the distributions of word frequencies?
What are the frequencies of 2-grams and 3-grams in the dataset?
How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
How do you evaluate how many of the words come from foreign languages?
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?
       
        TODO:  
         * Is it clean enough ? (should run some tests with regexp on pucntuation, symbols, numbers etc..)
         * Look at content through a simple word cloud
         * do some tf-idf explorations
```{r calculatiing term frequency} 
total_words <- corp_words %>%
                group_by(corp) %>%
                summarize(total=sum(n)) %>%
                arrange(total)

corp_words <- left_join(corp_words,total_words)

data(stop_words)
tf_idf_no_stop <- tf_idf %>% anti_join(stop_words)
library(wordcloud)
pal <- brewer.pal(5,"Greys")
wordcloud(tf_idf_no_stop[tf_idf_no_stop$corp=="blogs",]$word,
          tf_idf_no_stop[tf_idf_no_stop$corp=="blogs",]$tf,
          c(8,.5),
          max.words=30,
          colors=pal)
```

```{r some visual analysis}
##Let's graph what we have so far

g <- ggplot(corp_words,aes(n/total, fill=corp))
g <- g + geom_histogram(show.legend=FALSE) + xlim(NA,0.0009)
g <- g + facet_wrap(~corp,ncol=2, scales= "free_y")
g

```

