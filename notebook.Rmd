---
title: "Capstone Project for Data Science Specialization"
output: html_notebook
---

# Acquiring data

```{r}
        library(httr)
        url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
 if   (!dir.exists("./data")) {
      dir.create("./data")   
 }
 if(!file.exists("./data/swiftkey.zip")){
         GET(url,write_disk("./data/swiftkey.zip",overwrite = TRUE))
         unzip("./data/swiftkey.zip")
         }       
```
# Loading and tyding the data in R

```{r, message=TRUE, warning=TRUE, include=FALSE}
        library(dplyr)
        library(tidytext)
        blogs <- readLines("./data/final/en_US/en_US.blogs.txt")
        news <- readLines("./data/final/en_US/en_US.news.txt")
        twitter <- readLines("./data/final/en_US/en_US.twitter.txt")
        swears <- as.data.frame(readLines("./data/profanity_google.txt"))
        colnames(swears) <- c("text")
##Creating a sample corpus to try things out
blogs_df <- data_frame(corp="blogs",line=1:1000,text=sample(blogs,1000))
news_df <- data_frame(corp="news",line=1:1000,text=sample(news,1000))
twitter_df <- data_frame(corp="twitter",line=1:1000,text=sample(twitter,1000))

sample_corp <- rbind(blogs_df,news_df,twitter_df)
###

```
# Using tidy to make it nice
reference ["Text mining with R"](http://tidytextmining.com)
```{r}
library(dplyr)
library(tidytext)
library(tm)
# Maek it tidy, one word per row
# Some preprocessing
sample_corp$text <- tolower(sample_corp$text)
sample_corp$text <- removeNumbers(sample_corp$text) #if package not available, could use a regexp
sample_corp$text <- removePunctuation(sample_corp$text,preserve_intra_word_dashes = TRUE)
## Prepping swear list
tidy_swears <- as_tibble(as.character(swears))
colnames(tidy_swears) <- "word"
tidy_swears <- as_tibble(as.character(swears))
colnames(tidy_swears) <- "word"
##
corp_words <- sample_corp %>%
        unnest_tokens(word,text) %>%
        anti_join(tidy_swears) %>%
        group_by(word,corp) %>%
        summarize(n=n()) %>%
        ungroup()
total_words <- corp_words %>%
                group_by(corp) %>%
                summarize(total=sum(n))
#Renoving stopwords
#tidy_corp <- tidy_corp %>% 
#        anti_join(stop_words)



```
We can also make it a corpus in tm
``` {r}
        library(tm)
        mycorp <- Corpus(DirSource("./data/final/en_US"),readerControl =list(reader= readPlain, language="en_US") )
        summary(mycorp)
```
# Now some cleanup
## Cleaning up punctuation (easy enough)
```{r }
#Removing punctuation
corp <- tm_map(mycorp,removePunctuation)
corp <- tm_map(corp,content_transformer(tolower))
corp <- tm_map(corp,removeNumbers)
corp <- tm_map(corp,stripWhitespace)
```
## Removing profanity (less obvious)
source for list of word - a [gist](https://gist.github.com/ryanlewis/a37739d710ccdb4b406d)
```{r}
#Load a list of swear words (based on google banned workds)
swears <- readLines("./data/google_twunter_lol copy.txt")

```