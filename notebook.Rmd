---
title: "Capstone Project for Data Science Specialization"
output:
  html_document: default
  html_notebook: default
date: "Sep 11 2017"
---

# Acquiring data

```{r}
        library(httr)
        url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
 if   (!dir.exists("./data")) {
      dir.create("./data")   
 }
 if(!file.exists("./data/swiftkey.zip")){
         GET(url,write_disk("./data/swiftkey.zip",overwrite = TRUE))
         unzip("./data/swiftkey.zip")
         }       
```
# Loading and tyding the data in R

```{r, message=TRUE, warning=TRUE, include=FALSE}
        library(dplyr)
        library(tidytext)
        blogs <- readLines("./data/final/en_US/en_US.blogs.txt")
        news <- readLines("./data/final/en_US/en_US.news.txt")
        twitter <- readLines("./data/final/en_US/en_US.twitter.txt")
        swears <- as.data.frame(readLines("./data/profanity_google.txt"))
        colnames(swears) <- c("text")
##Creating a sample corpus to try things out
blogs_df <- data_frame(corp="blogs",line=1:1000,text=sample(blogs,1000))
news_df <- data_frame(corp="news",line=1:1000,text=sample(news,1000))
twitter_df <- data_frame(corp="twitter",line=1:1000,text=sample(twitter,1000))

sample_corp <- rbind(blogs_df,news_df,twitter_df)
###

```
# Using tidy to make it nice: ["Text mining with R"](http://tidytextmining.com)
## Loading data, pre-processing and creating tidy text data set with word counts
```{r loading data, echo=TRUE, message=TRUE}
library(dplyr)
library(tidytext)
library(tm)
library(ggplot2)
# Maek it tidy, one word per row
# Some preprocessing
sample_corp$text <- tolower(sample_corp$text)
sample_corp$text <- removeNumbers(sample_corp$text) #if package not available, could use a regexp
sample_corp$text <- removePunctuation(sample_corp$text,preserve_intra_word_dashes = TRUE)
## Prepping swear list
tidy_swears <- as_tibble(as.character(swears))
colnames(tidy_swears) <- "word"
tidy_swears <- as_tibble(as.character(swears))
colnames(tidy_swears) <- "word"
##
corp_words <- sample_corp %>%
        unnest_tokens(word,text) %>%
        anti_join(tidy_swears) %>%
        group_by(word,corp) %>%
        summarize(n=n()) %>%
        ungroup()
```
## Exploring our text data set
        TODO:  
         * Is it clean enough ? (should run some tests with regexp on pucntuation, symbols, numbers etc..)
         * Look at content through a simple word cloud
         * do some tf-idf explorations
```{r calculatiing term frequency} 
total_words <- corp_words %>%
                group_by(corp) %>%
                summarize(total=sum(n))

corp_words <- left_join(corp_words,total_words)
tf_idf <- corp_words %>%
        bind_tf_idf(word,corp,n) %>%
        arrange(desc(tf_idf))

data(stop_words)
tf_idf_no_stop <- tf_idf %>% anti_join(stop_words)
library(wordcloud)
pal <- brewer.pal(5,"Greys")
wordcloud(tf_idf_no_stop[tf_idf_no_stop$corp=="blogs",]$word,
          tf_idf_no_stop[tf_idf_no_stop$corp=="blogs",]$tf,
          c(8,.5),
          max.words=30,
          colors=pal)
```

```{r some visual analysis}
##Let's graph what we have so far

g <- ggplot(corp_words,aes(n/total, fill=corp))
g <- g + geom_histogram(show.legend=FALSE) + xlim(NA,0.0009)
g <- g + facet_wrap(~corp,ncol=2, scales= "free_y")
g

```

