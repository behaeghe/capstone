require(cmscu)
##Create dictionaries

twitter_1g <- new(FrequencyDictionary,4,2^26)
twitter_2g <- new(FrequencyDictionary,4,2^26)
twitter_3g <- new(FrequencyDictionary,4,2^26)
twitter_4g <- new(FrequencyDictionary,4,2^26)

# a text cleaning function
clean <- function(line) {
  # upper-case everything
  str <- tolower(line);
  # strip-out small html tags
  str <- gsub('<[^>]{1,2}>', '', str);
  # replace all terminal punctuation with a period
  str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '.', str);
  # get rid of anything not A-Z, ', ., or whitespace
  str <- gsub('[^a-z\'.[:space:]]', ' ', str);
  # crowd apostrophes
  # str <- gsub("[[:space:]]+([A-Z]*'[A-Z]*)", "\\1", str);
  # collapse whitespace
  str <- gsub('[[:space:]]+', ' ', str);
  # make sure contraction's are "tight"
  str <- gsub(" ?' ?", "'", str);
  # make sure terminal . are tight
  str <- gsub(' ?\\. ?', '.', str);
  return(str);
}

# this function lets us create n-grams from a list
ngrams <- function(lst, n) {
  len <- length(lst);
  sapply(1:(len-n+1), function(i) do.call(paste, as.list(lst[i:(i+n-1)])))
}


# connect to the file, but don't load the contents! 
twitterfile <- file('./final/en_US/data.train.80', 'r', FALSE); 
i <- 500
repeat {
  # select the number of reviews to read at a time. 500 = ~550kb. 
  tweets <- readLines(con=twitterfile, n=500);
  # Break loop when you reach the end of the file
  #if (i<(10**6/500) ){ #only loop through 1000 reviews for testing your loop
  if (length(tweets) == 0) { #comment out if you only want to test loop on first 1000 reviews
    # disconnect the file link
    close(twitterfile);
    # break the loop
    break;
  }
  
  # read a single review 
  for (tweet in tweets){
    # parse the current review
    curtweet <- tweet
    # clean the current review
    text <- clean(curtweet)
    # split reviews into sentences
    for (sentence in unlist(strsplit(text, '\\.'))) {
      # split to unigrams    
      unilist <- unlist(strsplit(sentence, ' ')) 
      # store unigrams
      twitter_1g$store(unilist)
      # add beginning and end of sentence tags to unigrams, and subsequent n-grams 
      # (crucial for smoothing ngrams in test phase)
      bilist <- c("<BOS>",unilist,'<EOS>')
      # store bigrams, use the "ngrams" function bind unigrams together
      twitter_2g$store(ngrams(bilist,2))
      # store trigrams    
      trilist <- c("<BOS>","<BOS>",unilist,'<EOS>')
      twitter_3g$store(ngrams(trilist,3))
      # store quadgrams
      qualist <- c("<BOS>","<BOS>","<BOS>",unilist,'<EOS>')
      twitter_4g$store(ngrams(qualist,4))
      i <- i + 500
    }
  }
  cat('\r', paste('Trained', i, 'lines from twitter.')); #this will track your progress through your dataset!
}  #else {break;}



