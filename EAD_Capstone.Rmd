---
title: 'Coursera Capstone EDA - Report #1'
author: "JRB"
date: "November 26, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align = "center")
  
library(tidyverse) ##Because the universe is a better place with tidyverse
library(tidytext)
library(tm)
library(ggraph)
library(igraph)
library(knitr) #For table formatting
library(gridExtra)
library(ggthemes)
## Formating for plots
ggstyle <- function() {
  theme_tufte() +
  theme(
                text = element_text(
                                        face="italic",
                                        colour="black", 
                                        size=10),
                plot.title = element_text(
                              face="bold",
                              colour="black",
                              size=11,
                              hjust = 0.5
                )
                        )

} 
## Formatting for graphs
a <- grid::arrow(angle=45,
                 type="open",
                 unit(4,"mm"))
```

##Introduction
This report presents a summary of findings from an exploratory analysis of the twitter data files provided for the captsone project. The goal of the exploratory data anlysis is to highlight interesting characteristics of the data that could be used to inform our modelling. The analysis has been performed on a sample of 20,000 lines from the twitter corpus.  
##Data Aquistion  

We are downloading and unziping the zip file provided by Coursera. We are also downloading a list of [profanities]("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en") to be used later to clean up the data set. We then proceed to create a data frame corp, which contains one observation per line of text and each observation as three variables: 
 
* *corp*: the origin of the text (twitter, news or blogs)    
* *line*: line number   
* *text*: the line of text itself from the text    

At the end of data acquisition we have a single data frame ```corp``` that would be used to performe our exploratory data analysis.

```{r Data Aquisition, warning=FALSE}
library(httr)
url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if   (!dir.exists("./data")) {
      dir.create("./data")   
 }
if(!file.exists("./data/swiftkey.zip")){
         GET(url,write_disk("./data/swiftkey.zip",overwrite = TRUE))
         unzip("./data/swiftkey.zip")
 }  
if(!file.exists("./data/profanity.txt")) {
        url = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
        GET(url,write_disk("./data/profanity.txt"))
}
        
blogs <- readLines("./final/en_US/en_US.blogs.txt")
news <- readLines("./final/en_US/en_US.news.txt")
twitter<- readLines("./final/en_US/en_US.twitter.txt")
## This is is our "profanity" list
swears <- readLines("./data/profanity.txt")
 ## Prepping our list of swear words
swears <- as_tibble(as.character(swears))
colnames(swears) <- "word"
##Creating a corpus for our exploratory data analysis
blogs_df <- data_frame(corp="blogs",line=1:length(blogs),text=blogs)
news_df <- data_frame(corp="news",line=1:length(news),text=news)
twitter_df <- data_frame(corp="twitter",line=1:length(twitter),text=twitter)
corp <- rbind(blogs_df,news_df,twitter_df)
###
```

##Data Preparation

A few simple steps to prepare the data for analysis.  
 
* Step 1: select the twitter data only
* Step 2: Normalize the case, clean any twitter artifacts (e.g. "rt") and remove any numbers in the text
* Step 3: tokenize the text by words and eliminateany profanity	

This should be enough preparation for our exploratory analysis 

```{r data preparation}
seed(2017)
#Step 1: subset for twitter
corp <-twitter_df
#corp <- sample_n(corp,20000)
#Step 2: normalizing case, cleaning twitter artifacts and removing digits
#corp$text <- tolower(corp$text) Not needed if using unnest_token
corp$text <- gsub("rt","",corp$text) #Remove rt, retweet
corp$text <- gsub("@\\w+", "", corp$text) #Remoeve any @
corp$text <- gsub("[[:digit:]]", "", corp$text) # remove numbers

#Step 3: removing stop words (a, the, etc...) and profanity
data(stop_words)
tidy_corp_nostop <- corp %>%
                unnest_tokens(word,text,token="words") %>%
                anti_join(swears, by="word") %>%
                anti_join(stop_words, by="word")

tidy_corp_stop <- corp %>%
                unnest_tokens(word,text,token="words") %>%
                anti_join(swears, by="word")

tidy_corp_nostop_summary <- tidy_corp_nostop %>% 
  group_by(corp) %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  bind_tf_idf(word,corp,n) 

#tidy_corp_nostop_summary <- tidy_corp_nostop_summary %>%  mutate(cumsum=cumsum(n)) %>% mutate(cumsumfreq=cumsum(tf))

tidy_corp_stop_summary <- tidy_corp_stop %>%
  group_by(corp) %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  bind_tf_idf(word,corp,n)

#tidy_corp_stop_summary <- tidy_corp_stop_summary %>%  mutate(cumsum=cumsum(n)) %>% mutate(cumsumfreq=cumsum(tf))

```


##Exploratory Data Analysis
### Summary Statistics
```{r as.is= TRUE}
  number_of_lines <- max(corp[corp$corp=="twitter",]$line) 
  number_of_words <- length(tidy_corp_nostop[corp$corp=="twitter",]$word)
  distinct_words <- length(tidy_corp_nostop_summary[corp$corp=="twitter",]$word)
  mySummary <- data.frame(lines=number_of_lines,words=number_of_words,distinct=distinct_words)
  #
  number_of_lines <- max(corp[corp$corp=="blogs",]$line) 
  number_of_words <- length(tidy_corp_nostop[corp$corp=="blogs",]$word)
  distinct_words <- length(tidy_corp_nostop_summary[corp$corp=="blogs",]$word)
  tmp <- c(lines= number_of_lines,words=number_of_words,distinct=distinct_words)
  mySummary <- rbind(mySummary,tmp)
  #
  number_of_lines <- max(corp[corp$corp=="news",]$line) 
  number_of_words <- length(tidy_corp_nostop[corp$corp=="news",]$word)
  distinct_words <- length(tidy_corp_nostop_summary[corp$corp=="news",]$word)
  tmp <- c(lines= number_of_lines,words=number_of_words,distinct=distinct_words)
 
  mySummary <- rbind(mySummary,tmp)
  row.names(mySummary) <- c("twitter","blogs","news") 
  kable(mySummary)
```

##Part 1: monogram analysis  

```{r word count plot, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
#filtering for twitter only

corp_freq <- tidy_corp_stop_summary %>% 
              filter(corp=="twitter") %>% 
              arrange(desc(tf)) %>% 
              mutate(word=factor(word,levels=word)) %>%
              mutate(cumsum=cumsum(tf)) 
          
g1 <- corp_freq %>% filter(rank(desc(tf))<=25) %>%
  ggplot(aes(reorder(word,desc(word)), tf)) +
  geom_point() +
  xlab(NULL) +
  ylab("Frequency") +
  ggtitle("25 most frequent words \n including stop words") +
  coord_flip() + 
  ggstyle()
  
corp_freq_nostop <- tidy_corp_nostop_summary %>% 
                  filter(corp=="twitter") %>% 
                  arrange(desc(tf)) %>% 
                  mutate(word=factor(word,levels=word)) %>%
                  mutate(cumsum=cumsum(tf)) 

g2 <- corp_freq_nostop %>%  filter(rank(desc(tf))<=25) %>%
  ggplot(aes(reorder(word,desc(word)), tf)) +
  geom_point() +
  xlab(NULL) +
  ylab("Frequency") +
  ggtitle("25 most frequent words \n excluding stop words") +
  coord_flip() +
  ggstyle()
  grid.arrange(g1,g2,ncol=2)
```
--

###Word cloud
Word cloud showing the relative importance of the 125 most frequent words in the twiter corpus

```{r word cloud}
library(wordcloud)
#freq_word <- bind_tf_idf(count_word,word,orig,n)
wordcloud(corp_freq_nostop$word,freq=corp_freq_nostop$tf,max.words=125,colors=brewer.pal(9,"RdPu"))
```
--

##Part 2: n-gram analysis


###Bigrams analysis  
```{r 2-grams}

corp_bigram <- unnest_tokens(corp[corp$corp=="twitter",],ngram,text,token="ngrams",n=2)
corp_bigram <- corp_bigram %>%
          separate(ngram,c("word1","word2"),sep=" ")
#corp_bigram <- corp_bigram %>%
#              filter(!word1 %in% stop_words$word) %>%
#              filter(!word2 %in% stop_words$word)
corp_bigram_count <- corp_bigram %>%
          unite(bigram,word1, word2, sep=" ") %>%
          count(bigram, sort=TRUE) %>%
          mutate(orig="twitter") %>%
          arrange(desc(n))
## TO DO: Bigram are factors, need to either make it a string or re-order the factors.
corp_bigram_count <- corp_bigram_count %>% 
                    bind_tf_idf(bigram,orig,n) %>%
                    mutate(bigram=factor(bigram,levels=bigram))
bigram_graph <- corp_bigram_count %>% 
                filter(rank(desc(n)) <= 10) %>%
                graph_from_data_frame()
g1 <-  corp_bigram_count %>% 
  filter(rank(desc(tf))<= 25) %>%
  ggplot(aes(reorder(bigram,desc(bigram)),tf)) +
  geom_point(stat="identity") + 
  xlab(NULL) +
  coord_flip()+
  ylab("Frequency") +
  ggtitle("25 most frequent bigrams") +
  ggstyle()
g1

#bigram

bigram_graph <- corp_bigram %>% 
                count(word1,word2,sort=TRUE) %>%
                filter(rank(desc(n)) <= 125) %>%
                graph_from_data_frame() 
#a <- grid::arrow(type="closed",unit(0.15,"inches"))
ggraph(bigram_graph, layout = "fr") +
      geom_edge_link(aes(edge_alpha=n),
                  show.legend = FALSE,
                  arrow=a,end_cap=circle(0.07,"inches") ) +
      geom_node_point(color="lightblue",size=5) +
      geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
      theme_graph() +
      ggtitle("Connected graph showing the 125 most frequents bigrams") +
      theme(
        plot.title= element_text(
          face="bold",
          size=10,
          hjust=0.5
        )
      )
      
```

###Trigrams Analysis   

```{r 3-grams}
corp_trigram <- unnest_tokens(corp[corp$corp=="twitter",],ngram,text,token="ngrams",n=3)
corp_trigram <- corp_trigram %>%
          separate(ngram,c("word1","word2","word3"),sep=" ")
corp_trigram_count <- corp_trigram %>%
          unite(trigram,word1, word2,word3, sep=" ") %>%
          count(trigram, sort=TRUE) %>%
          mutate(orig="twitter") %>%
          arrange(desc(n))
corp_trigram_count <- corp_trigram_count %>% 
                    bind_tf_idf(trigram,orig,n) %>%
                    mutate(trigram=factor(trigram,levels=trigram))

g1 <-  corp_trigram_count %>% 
  filter(rank(desc(tf))<= 25) %>%
  ggplot(aes(reorder(trigram,desc(trigram)),tf)) +
  geom_point(stat="identity") + 
  xlab(NULL) +
  coord_flip()+
  ylab("Frequency") +
  ggtitle("25 most frequent trigrams") +
    theme_minimal() +
  theme(
                text = element_text(
                                        face="italic",
                                        colour="black", 
                                        size=8)
)
g1

trigram_graph <- corp_trigram %>% 
                count(word1,word2,word3,sort=TRUE) %>%
                filter(rank(desc(n)) <= 125) %>%
                graph_from_data_frame() 
#a <- grid::arrow(type="closed",unit(0.15,"inches"))
ggraph(trigram_graph, layout = "fr") +
      geom_edge_link(aes(edge_alpha=n),
                  show.legend = FALSE,
                  arrow=a,end_cap=circle(0.07,"inches") ) +
      geom_node_point(color="lightblue",size=5) +
      geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
      theme_graph() +
      ggtitle("Connected graph showing the 125 most frequents trigrams") +
      theme(
        plot.title= element_text(
          face="bold",
          size=10,
          hjust=0.5
        )
      )

```
```{r graph analysis, eval=FALSE, include=FALSE}
#bigram

bigram_graph <- corp_bigram %>% 
                count(word1,word2,sort=TRUE) %>%
                filter(rank(desc(n)) <= 125) %>%
                graph_from_data_frame() 
#a <- grid::arrow(type="closed",unit(0.15,"inches"))
ggraph(bigram_graph, layout = "fr") +
      geom_edge_link(aes(edge_alpha=n),
                  show.legend = FALSE,
                  arrow=a,end_cap=circle(0.07,"inches") ) +
      geom_node_point(color="lightblue",size=5) +
      geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
      theme_void()
#Trigram


trigram_graph <- corp_trigram %>% 
                count(word1,word2,word3,sort=TRUE) %>%
                filter(rank(desc(n)) <= 125) %>%
                graph_from_data_frame() 
#a <- grid::arrow(type="closed",unit(0.15,"inches"))
ggraph(trigram_graph, layout = "fr") +
      geom_edge_link(aes(edge_alpha=n),
                  show.legend = FALSE,
                  arrow=a,end_cap=circle(0.07,"inches") ) +
      geom_node_point(color="lightblue",size=5) +
      geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
      theme_void()
```
###Dictionary Coverage  
As illustrated in the graph below, based on the corpus containing stop words, it will take a dictionary of `r count(corp_freq[corp_freq$cumsum <=0.8,])` words to cover 80% of the words in the corpus (blue lines) and a `r count(corp_freq[corp_freq$cumsum <=0.5,])` words dictionary to cover 50% of the corpus (red lines).  

```{r graph for dictionary coverage}
cutoff0.5 <- count(corp_freq[corp_freq$cumsum <=0.5,])$nn
cutoff0.8 <- count(corp_freq[corp_freq$cumsum <=0.8,])$nn
plot(corp_freq$cumsum, type="l",xlab="word count",ylab="cumulative sum")
abline(h=0.5,col="red")
abline(v=cutoff0.5,col="red")
abline(h=0.8,col="blue")
abline(v=cutoff0.8,col="blue")

```
  
###Conclusions and Next Steps
The analysis shows that a limited dictionary can be used to ensure a relatively large coverage of the corpus. The connected n-grams graph could be used to build the models to asses the probaility of the next word given a starting word. For example, one could imagine a shiny app where the end user enter a word and the application would suggest the most 3 probables words based on the connected graph we could derive from the corpus.  

##References:  
[How to clean twitter data in r](http://technokarak.com/how-to-clean-the-twitter-data-using-r-twitter-mining-tutorial.html)  
[Text mining in R: the tidy approach](http://tidytextmining.com/)